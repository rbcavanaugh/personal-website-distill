---
title: "Analyzing single-subject design studies using Bayesian hierarchical models"
description: |
  A method for leveraging Bayesian methodology to estimate individual effect sizes in single-subject designs is discussed in detail. 
author:
  - name: Rob Cavanaugh
date: 1-26-2021
categories:
  - stats
  - aphasia treatment
output:
  distill::distill_article:
    self_contained: true
    toc: true
    toc_depth: 4
draft: true
bibliography: citations.bib
nocite: | 
  @wiley2018, @burkner2018, @nalborczyk2019, @burkner2017, @creswell2011best, @mcelreath2020statistical
---

```{r setup, include=FALSE}
library(here) ##### add figure and table captions
library(knitr)
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

## Introduction

This document describes procedures for Bayesian multilevel modeling of single-subject design treatment data. Recommended reading prior to reviewing this document: <https://easystats.github.io/bayestestR/articles/bayestestR.html>. Another accessible source is here: <https://m-clark.github.io/bayesian-basics/intro.html>[@bayestestR]. Excepts from these vignettes follows:

> Adopting the Bayesian framework is more of a shift in the paradigm than a change in the methodology. Indeed, all the common statistical procedures (t-tests, correlations, ANOVAs, regressions, ...) can be achieved using the Bayesian framework. One of the core difference is that in the frequentist view (the "classic" statistics, with p and t values, as well as some weird degrees of freedom), the effects are fixed (but unknown) and data are random. On the other hand, in the Bayesian inference process, instead of having estimates of the "true effect", [the probability of different effects given the observed data is computed]{.ul}, resulting in a distribution of possible values for the parameters, called the posterior distribution.

From Michael Clark:

> To summarize conceptually, we have some belief about the state of the world, expressed as a mathematical model (such as the linear model used in regression). The Bayesian approach provides an updated belief as a weighted combination of prior beliefs regarding that state and the currently available evidence, with the possibility of the current evidence overwhelming prior beliefs, or prior beliefs remaining largely intact in the face of scant evidence.

```{r, fig.cap="Informative Prior from https://micl.shinyapps.io/prior2post/", out.extra="class=external", echo=F}
knitr::include_graphics("prior1.png")
```

## Prior Beliefs

Exercise: assign prior distributions to the following:

1. Average performance during the baseline phase for the PPA study
1. The probability of any participant with PPA naming a random trained item correctly after treatment
1. The standard deviation of performance during the baseline phase

How much prior distributions influence findings depends on the certainty assigned to these distributions and the amount of data that composes the likelihood. Often, we don't want to influence any effects, so we assign priors that are composed of reasonable values, but centered around zero. Assigning a prior distribution of reasonable values often helps model convergence. In these cases, findings from Bayesian and frequentist models typically converge around the same result.

```{r, fig.cap="Weakly Informative Prior. Figure from https://micl.shinyapps.io/prior2post/", out.extra="class=external", echo=F}
knitr::include_graphics("prior2.png")
```

## Posterior Distributions

Key takeaway: The Bayesian way of thinking results in a distribution of possible values for a given effect (our updated belief, the prior distribution). There are a few big advantages of the posterior distribution rather than a test-statistics or p-value. 

First, we can calculate how much of this distribution exceeds some value, often called the posterior probability. For example, the posterior probability that an effect is greater than 0 is found by calculating the amount of the distribution that is greater than zero. While p-values impose an arbitrary cutoff (and leave us wondering how to interpret p = 0.06), the posterior probability lets us think on a continuous probability about our results. 

Second, and related, Bayesian modeling returns the Bayesian credible interval in lieu of a confidence interval. The credible interval is a range containing a given percentage of the posterior distribution. 

**Credible Interval:** *"Given the observed data, the effect has 95% probability of falling within this range" [@bayestestR].*

**Confidence interval:** *"There is a 95% probability that when computing a confidence interval from data of this sort, the effect falls within this range"[@bayestestR].*

In the mixed-effects context, Bayesian models return posterior distributions for *every* model parameter (fixed and random) as well as for model predictions. A refresher on mixed-effects models: http://mfviz.com/hierarchical-models/. 

## Setup

### Packages

```{r packages, warning=F, message=F, echo = T}
# load packages
library(knitr)
library(here) # if you don't use rprojects and here() already, you should start...
library(tidyverse)
library(brms)
library(cmdstanr)
library(tidybayes)
library(bayesplot)
library(ggthemes)
library(bayestestR)
library(janitor)
library(lme4)
```

### Read in Data

Note, the data is in long format: one row for each naming attempt. 

```{r read, warning=F, message=F, echo = T}
# read in data
df <- read_csv('~/Desktop/SDTBI001_LongData.csv') %>%
  clean_names() %>%
  mutate(condition = ifelse(condition == 0, 'untreated', 'treated'),
         phase = as.factor(phase)) %>%
  mutate_if(is.character, as.factor) 

kableExtra::kable(head(df, 10), caption = "Preview of item-level data structure")
```

### Preview Data

Aggregate performance for 5 participants over 5 baseline and 10 treatment probe sessions. The simulation was run with 30 'words' total which were selected so that the average baseline performance across 500 participants was roughly 30%.


```{r visualize, warning = F, message = F, echo = T, fig.cap="Performance over time for treated and untreated words"}

df %>%
  group_by(condition, session, phase) %>%
  summarize(num_correct = sum(response)) %>%
  ggplot(aes(y = num_correct, x = session, color = condition, shape = phase)) +
  geom_point(size = 3) +
  geom_line(size = .8) +
  geom_vline(xintercept = 3.5, linetype = 'dashed') +
  geom_vline(xintercept = 10.5, linetype = 'dashed') +
  geom_vline(xintercept = 12.5, linetype = 'dashed') +
  geom_vline(xintercept = 13.5, linetype = 'dashed') +
  geom_vline(xintercept = 14.5, linetype = 'dashed') +
  scale_x_continuous(breaks = seq(1,15,1), limits = c(1,15)) +
  scale_y_continuous(breaks = seq(0,60,5), limits = c(0,60)) +
  theme_tidybayes()

```

## Interrupted time series model

Now, we're going to implement an interrupted time series model [@huitema2000], which models performance as a function of the slope during the baseline phase, level change between the last baseline and first treatment session, and the change in slope between baseline and treatment:

$$
\\Y_t=\beta_0+\beta_1T_t+\beta_2D_t+\beta_3[T_t-(n_1+1)]D_t+\epsilon_t\
$$

Here, $\beta_0$ (the intercept) describes performance prior to the first baseline session. $\beta_1$ is our session variable, in this case 1, 2, 3...n where n is the total number of sesions. $\beta_2$ is the level change variable, where baseline sessions are coded as 0 and treatment sessions are coded as 1. $\beta_3$ represents the change in slope, the product of the level change variable and the session variable centered around the first treatment session.  $\beta_3$ is thus coded as 0 during baseline and the treatment sessions are coded as 0, 1, 2....n where n is the number of treatment sessions. 

Note: I've considered the post-treatment time points an extension of the treatment variables for simplicity, but we should have a discussion about how to treat these timepoints. 

```{r}
df_coded <- df %>%
  filter(session < 13) %>%
  mutate(phase = ifelse(phase == 'Pre', 0, 1),
         session.cent = session - 4,
         slope_change = session.cent*phase
         ) %>%
  select(id, set, condition, response, session, phase, session.cent, slope_change)

kable(head(df_coded, 15), caption = "Data structure for interrupted time series model")
```

### Implementation

In this example, we'll implement the model for one participant. If the number of participants is 9 or 10 or more, it becomes more reasonable to utilize a group-level model, which is demonstrated further below.

In 'frequentist' world (e.g. lme4), the code for the above model is as follows:

```{r lme4, cache = T, eval = F}

glmer_model <- glmer(response ~ session + phase + slope_change + 
                       (session + phase + slope_change | id),
           family = 'binomial',
           data = df_coded %>% filter(condition == 'treated')
)

```


In the BRMS package, the syntax is effectively the same, with a few added arguments.

```{r model fitting,  message = F, echo = T, cache = T}
# Model Fitting
# weakly informative priors assist model estimation

m.1 <- brm(response ~ 0 + Intercept + session + phase + slope_change + (session + phase + slope_change | id),
           family = bernoulli(),  #binomial in lme4
           data = df_coded %>% filter(condition == 'treated'),
           iter = 4000, # number of total samples per chain
           warmup = 1000, # number of samples tossed out per chain
           inits = 'random', # each chain takes random starting values
           chains = 4, # 4 total chains
           cores = 4, # use 4 computer cores to run chains simultaneously,
           control = list(adapt_delta = .9), # to aid model convergence
           prior = c(prior(cauchy(0, 5), class = b), # prior distributions!
                     prior(cauchy(0, 5), class = sd)),
           sample_prior = T,
           backend = 'cmdstan',
           seed = 42,
           refresh = 0 #this is only for the markdown document. 
)

```

What are these prior distributions? The cauchy distribution is similar to a normal distribution, but with 'fatter' tails. I've chosen the cauchy here as coefficients in binary logistic regression can be larger than normal gaussian regression. The prior distribution applied to the standard deviations in the model is technically a 'half cauchy' which reflects the fact that standard deviation values will only be positive. I have set the same priors to all beta-values and standard deviations, but they can also be set individually. See all options by calling prior_summary()

```{r prior-sum}

kable(prior_summary(m.1), caption = "Default priors for our model")

```

This choice reflects my prior beliefs about the expected results: I am expecting all parameter estimates to easily fall within 0 +/- 10 logits. If they fall outside of this range, that would be highly unexpected. For example, an average baseline performance of 33% would correspond to a logit of roughly -0.7. An intercept of -10 would correspond to an average baseline performance that is effectively 0 (specifically, 0.0005), which is possibly, but unlikely given the way this study has designed stimuli selection and inclusion/exclusion criteria. A logit of 10 would correspond to baseline performance that is approaching 1. The result here is that the prior distributions are considered 'weakly informative.' They will not influence the posterior distribution other than to aid model convergence by influence the range of values that they model considers possible, and probable.  

```{r cauchy, fig.cap="Three possibilities for assigning priors"}

tibble(cauchy = rcauchy(10000,0,5),
       normal = rnorm(10000,0,5),
       uniform = runif(10000, -30,30)
) %>%
  pivot_longer(cols = 1:3, names_to = 'distribution', values_to = 'val') %>%
  ggplot(aes(x = val, color = distribution, fill = distribution)) + 
  geom_density(alpha = .25, scale = 2) +
  xlim(-25,25) + theme_tidybayes()

```

### Model Evaluation Steps:
  
Here are some things you should do to evaluate whether the model converged before peeking at results. 
  
Posterior Chains: these should look like a lot of noise, which they do. 
  
```{r evaluate-model, warning = F, message = F, echo = T, fig.cap="MCMC trace plots. An unhelpful heuristic: it should look lik a hairy catepillar"}
# model evaluation step #1: evaluate chains
mcmc_plot(m.1, type = 'trace')
```

Effective Sample Size: these should be over 1000 per. Some are a little low. we could resolve this by increasing the number of iterations in our model. 
  
```{r ess, warning = F, message = F, echo = T}
# check that there is a sufficient effective sample size for each parameter 
# (>400 minimum; over 1000 ideal): effective samples are cutting it a little 
# close, it may be worth increasing the number of model 
# iterations to ensure parameter stability

effective_sample(m.1, effects = "all") %>%
  arrange(ESS) %>%
  head(5) %>%
  kable(caption = "Lowest effect sample sizes")
```

Split-half Reduction Scale (rhat): absolutely need to be less than 1.05, which they are. 
  
```{r rhat, warning = F, message = F, echo = T, fig.cap = "Visualizing Rhat values"}
# check rhat values are less than 1.05:
mcmc_plot(m.1, type = "rhat")
```

Evaluate Model Fit:the dark blue line (observed values) should be in the middle of the light grey lines (predicted values of the posterior distribution). Looks good. 
  
```{r ppcheck1, warning = F, message = F, echo = T, fig.cap = "Posterior predictive check"}
# this looks good
pp_check(m.1, nsamples = 100)
```

Having decided that our model is reasonable, we can interpret it. You can call summary(m.1) for a quick look. I've cobbled together this function to show the added value of thinking about effects in terms of distributions. Instead of dealing with insignificant p-values, we can ask "whats the probability this effect is greater than 0?" which is the posterior probability here. Another advantage is that we could define whats called the "ROPE" or the region of practical equivalence. Lets say we were interested in whether or not the intercept was meaningfully different from zero. We could define what 'meaningfully' means to us (lets say we consider 45% - 55% practically no different than 50%), and then calculate the probability the intercept falls outside this range. Perhaps this range defines a range of results that would not be clinically meaningful even if they were significant - we could ask for the probability that the effect we find falls outside of this range. 

Anyways, the 90% credible interval for all three parameters (excluding the intercept) includes zero. there is weak to modest evidence that these parameters are greater than zero. What's probability holding up this model is that the baseline phase only has 3 observations, and so the model is having a difficult time estimating the session slope, which has trickle-down effects on estimating phase and slope_change. The model likely characterizes change relatively well, it just doesn't give us a ton of confidence that changes are due to treatment. Looking at the plot above, I think we can be pretty confident these changes are due to treatment. Probably the major drawback of the interrupted time series approach - and it may not be the best way to test for changes in level or slope with so few baseline sessions.   

```{r summ}

model_summary_with_posteriors <- function(model_name){
posterior_samples(model_name)[,1:4] %>%
  summarize(across(.fns = function(x){val = mean(ifelse(x>0,1,0))})) %>%
  t() %>%
  as_tibble() %>%
  rename(posterior_probability = V1) %>%
  bind_cols(as_tibble(fixef(model_name, probs = c(0.05, .95)),
                      rownames='Parameter')) %>%
  select(Parameter, Estimate, Est.Error, Q5, Q95, posterior_probability) 
} 
 
 model_summary_with_posteriors(m.1) %>%
  kable(digits = 2, caption = "M.1 model summary")

```

### Effect Sizes

Let's calculate an effect size anyway.   We've been thinking about effect sizes in very practical terms. How many words did the participant gain as a result of treatment? Here's how we get there using this model: 

Remember how bayesian models provide a posterior distribution for each parameter in the model? Well that includes all levels of the random effects. So we can extract the models' posterior distributions which reflect whether or not each word is produced correctly at the end of baseline (session 3) and the end of treatment (session 15). The number of samples in each posterior distribution (for each word) is the number of iterations included in the model (12,000!). For each iteration, we subtract the value at session 3 from the value at session 15. The resulting distribution is the posterior distribution of the difference in performance between session 15 and session 3. The median of that distribution is the models best guess at the effect size. 


```{r}
# setup an empty df to make predictions from:
pred_d <- m.1$data %>% # this is the data that went into the model
  select(-response, -Intercept) %>% # we don't need these
  distinct() %>% # only unique rows
  filter(session == 3 | session ==12) # we only want these two timepoints

# adds the model distrbutions for each word at sessions 3 and 12
es <- add_fitted_draws(m.1, newdata = pred_d, pred = 'value', seed = 42) %>% 
  ungroup() %>% # ensure no grouping in the data
  mutate(time_point = ifelse(session == 3, 'entry', 'exit')) %>% # rename these time points
  select(time_point, id, value = .value, draw = .draw) %>% # select entry and exit
  group_by(draw, time_point) %>% # for each sample and time point
  summarize(num_corr = sum(value)) %>% # count the number of correct words predicted
  pivot_wider(names_from = time_point, values_from = num_corr) %>% # wider to substract
  mutate(effect_size = exit - entry) %>% # subtract
  ungroup() %>%  # remove grouping
  select(effect_size) 

es %>% # only need one variable
  summarize(effect_size = median(effect_size)) %>%
  kable(digits = 2)
```

This approach therefore suggests that this participant improved by roughly 48 words in response to treatment. But we could have calculated that by subtracting the number correct between session 12 and session 3 right? Yup:

```{r sum}
m.1$data %>%
  group_by(session) %>%
  summarize(num = sum(response)) %>%
  slice(3, 12) %>%
  summarize(change = num-lag(num)) %>%
  slice(2) %>%
  kable()

```

First, this is a nice sanity check. But, remember that we calculated the median of a very large distribution. So we can calculate a credible interval around our median estimate:

```{r}
# setup an empty df to make predictions from:
pred_d <- m.1$data %>% # this is the data that went into the model
  select(-response, -Intercept) %>% # we don't need these
  distinct() %>% # only unique rows
  filter(session == 3 | session ==12) # we only want these two timepoints

es %>% # only need one variable
  median_qi(.width = .89) %>%
  kable(digits = 2)
```

And we can plot it. 

```{r plot, fig.cap = "Distribution of effect size estimates for treated words"}
es %>%
  ggplot(aes(x = effect_size)) +
  stat_halfeye() +
  theme_tidybayes()
```
### Untreated Condition

What do we do about the untreated condition? In this case, the simplest thing is probably to run another model just for untreated items. 

```{r modelfitting2, message = F, echo = T, cache = T, eval = T}
# Model Fitting
# weakly informative priors assist model estimation

m.1.untreated <- brm(response ~ 0 + Intercept + session + phase + slope_change +
                       (session + phase + slope_change | id),
           family = bernoulli(),  #binomial in lme4
           data = df_coded %>% filter(condition == 'untreated'),
           iter = 4000, # number of total samples per chain
           warmup = 1000, # number of samples tossed out per chain
           inits = 'random', # each chain takes random starting values
           chains = 4, # 4 total chains
           cores = 4, # use 4 computer cores to run chains simultaneously,
           control = list(adapt_delta = .9), # to aid model convergence
           prior = c(prior(cauchy(0, 5), class = b), # prior distributions!
                     prior(cauchy(0, 5), class = sd)),
           sample_prior = T,
           seed = 42,
           backend = 'cmdstan',
           refresh = 0
)

```
```{R sumun}
 model_summary_with_posteriors(m.1.untreated) %>%
  kable(digits = 2, caption = "M.1.untreated model summary")
```

And calculate the estimate effect size for this model

```{r, fig.cap = "Distribution of effect size for untreated words"}
# setup an empty df to make predictions from:
pred_d <- m.1.untreated$data %>% # this is the data that went into the model
  select(-response, -Intercept) %>% # we don't need these
  distinct() %>% # only unique rows
  filter(session == 3 | session ==12) # we only want these two timepoints

# adds the model distrbutions for each word at sessions 3 and 12
es.un <- add_fitted_draws(m.1.untreated, newdata = pred_d, pred = 'value', seed = 42) %>% 
  ungroup() %>% # ensure no grouping in the data
  mutate(time_point = ifelse(session == 3, 'entry', 'exit')) %>% # rename these time points
  select(time_point, id, value = .value, draw = .draw) %>% # select entry and exit
  group_by(draw, time_point) %>% # for each sample and time point
  summarize(num_corr = sum(value)) %>% # count the number of correct words predicted
  pivot_wider(names_from = time_point, values_from = num_corr) %>% # wider to substract
  mutate(effect_size = exit - entry) %>% # subtract
  ungroup() %>%  # remove grouping
  select(effect_size) 

es.un %>% 
  ggplot(aes(x = effect_size)) +
  stat_halfeye() +
  theme_tidybayes()
```

## A simpler Pre-Post Approach

Given the number of observations here, a simpler approach might be to simply compare the difference in accuracy between the three pre-treatment sessions and the two post-treatment sessions. In this case, we're going to assume that there is no meaningful baseline slope that we need to account for (though, given our results above, it's hard to account for it with 3 baseline sessions anyway). The benefit is that we can model treated and untreated words at the same time. 

### Implementation 

``` {r prepost,  message = F, echo = T, cache = T, eval = T}

# A littl bit of datawrangling
df.mod2 <- df %>%
  filter(phase == 'Pre' | phase == 'Post') %>% # only pre and post timepoints
  droplevels() %>%
  mutate(phase = as.factor(phase), # phase as factor
         # condition as a factor with labels
         condition = as.factor(condition)) 

# pre as reference (detect differences between performance at baseline)
contrasts(df.mod2$phase) <- c(1,0) 

# treated as reference (detect whether treated items change in response to treatment)
contrasts(df.mod2$condition) <- c(0,1)

# Simple interaction model
m.2 <- brm(response ~ 0 + Intercept + phase*condition + 
             (phase|id),
           family = bernoulli(),
           iter = 2000,
           warmup = 1000,
           inits = 'random',
           prior = c(prior(cauchy(0, 5), class = b),
                      prior(cauchy(0, 5), class = sd)),
           backend = 'cmdstan',
           cores = 4,
           chains = 4,
           seed = 42,
           control = list(adapt_delta = .99),
           data = df.mod2,
           sample_prior = T,
           refresh = 0
)

```

### Model Evaluation Steps

A few quick posterior checks: 

```{r m2checks1, fig.cap = "MCMC trace plot"}
mcmc_plot(m.2, type = 'trace')
```

```{r essm2}
effective_sample(m.2, effects = "all") %>%
  arrange(ESS) %>%
  head(5) %>%
  kable(caption = "Lowest effect sample sizes for m.2")
```

### Model Refitting

This perhaps explains why the traceplot looks different for the random effect correlation, let's increase the number of iterations

``` {r prepost2, message = F, echo = T, cache = T, eval = T}

# Simple interaction model
m.2 <- brm(response ~ 0 + Intercept + phase*condition + 
             (phase|id),
           family = bernoulli(),
           iter = 5000, # now 4000
           warmup = 1000,
           inits = 'random',
           prior = c( prior(cauchy(0, 5), class = b),
                      prior(cauchy(0, 5), class = sd)),
           backend = 'cmdstan',
           cores = 4,
           chains = 4,
           seed = 42,
           control = list(adapt_delta = .99),
           data = df.mod2,
           sample_prior = T,
           refresh=0
)

```

```{r ess2m2}
effective_sample(m.2, effects = "all") %>%
  arrange(ESS) %>%
  head(5) %>%
  kable(caption = "Lowest effect sample sizes after increasing iterations")
```

much better. looking at the traceplot shows the improvement as well

```{r m2checks2,  fig.cap = "MCMC trace plot"}
mcmc_plot(m.2, type = 'trace')
```
And checking rhat values shows that the model has certainly converged. 

```{r rhat2, fig.cap = "Rhat values"}
mcmc_plot(m.2, type = "rhat")

```
And last lets check that the model fits the data well...looks lovely

```{r m2checks, fig.cap = "Posterior predictive check"}
brms::pp_check(m.2, nsamples = 100)
```

For simpler models, visualizing is actually quite simple:

```{r conditionaleffectsm2, fig.cap = "Probability correct for trained and untrained words with 89% credible intervals"}
plot(conditional_effects(m.2, prob = .89), plot = F)[[3]] + 
  scale_x_discrete(limits = c('Pre', 'Post')) +
  labs(y = "Probability of a correct response")
```


Because some coefficients are negative, the previous function for estimating posterior probability would need some tweaking, so we can just call a default funciton now. You can also call summary(model_name) for quick results (it doesn't format well in rmarkdown documents)

```{r sum2}
broom.mixed::tidyMCMC(m.2, prob = .89, conf.int = T, conf.level = .90) %>% slice(1:7) %>%
  kable(digits = 2, caption = "M.2 model summary")
```

Interesting that the main effect of condition is large and robust. This is due to non-linear effect sizes on the odds-ratio scale. Because the untreated items have 0 correct during baseline, the beta-coefficient is quite large. This makes sense - the odds of a correct response at baseline is substantially larger in the treated condition because the odds of a correct response in the untreated condition are effectively zero. 

Otherwise, the model suggests large and robust improvements for treated items, and that change is meaningfully different than the untreated items. 

### Effect Sizes

We can now estimate the effect size again for treated items like so:

``` {r m2effectsizes}

pred_d <- m.2$data %>%
  select(-response, -Intercept) %>%
  distinct()

es2 <- add_fitted_draws(m.2, newdata = pred_d, pred = 'value', seed = 42) %>% 
  ungroup() %>%
  select(phase, condition, id, value = .value, draw = .draw) %>% # select entry and exit
  group_by(draw, phase, condition) %>% # for each sample and time point
  summarize(num_corr = sum(value)) %>% # count the number of correct words predicted
  pivot_wider(names_from = phase, values_from = num_corr) %>% # wider to substract
  mutate(effect_size = Post - Pre) %>% # subtract
  group_by(condition) %>%  # remove grouping
  select(effect_size, condition) # only need one variable
  

es %>%
  median_qi(.width = .9) %>%
  kable(digits = 2)
```

```{r esvis2, fig.cap = "Distribution of effect sizes for pre-post model"}
es2 %>%
  ggplot(aes(x = effect_size, color = condition)) +
  stat_halfeye() +
  theme_tidybayes()

```

## Comparing effect size estimates between approaches:

It turns out that this estimate is very close to the estimate derived from the interrupted time series approach (47.8 vs. 48.6) with pretty similar credible intervals (44.3 - 50.7 vs. 44.7 to 51.8)

``` {r escompare, fig.cap = "comparing effect size estimates between ITTS and pre-post model structures for treated words"}

es_itts = es %>% mutate(method = 'ITTS')
es_interaction = es2 %>% filter(condition == 'treated') %>% mutate(method = 'Interaction')

bind_rows(es_itts, es_interaction) %>%
  ggplot(aes(x = effect_size, color = method, fill = method)) +
  stat_halfeye(alpha = .3) +
  theme_tidybayes()

```

Here's a little perspective:

``` {r escompare2, fig.cap = "Zoomed out comparison"}

es_itts = es %>% mutate(method = 'ITTS')
es_interaction = es2 %>% filter(condition == 'treated') %>% mutate(method = 'Interaction')

bind_rows(es_itts, es_interaction) %>%
  ggplot(aes(x = effect_size, color = method, fill = method)) +
  stat_halfeye(alpha = .3) +
  theme_tidybayes() +
  xlim(0,60)

```


## Resources I found very helpful

-   [Statistical Rethinking, tidyverse edition](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/)
-   [Tutorials from Rens van de Schoot](https://www.rensvandeschoot.com/tutorials/brms-started/)
-   [The EasyStats packages and BayestestR](https://easystats.github.io/bayestestR/)
-   [Vignettes from Paul-Christian Bürkner](https://paul-buerkner.github.io/brms/)
-   [Guide to Stan warnings](https://mc-stan.org/misc/warnings.html)







